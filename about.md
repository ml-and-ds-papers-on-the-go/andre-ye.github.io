[Home](https://andre-ye.github.io/) > About

## About ü§ô

**Hey üëã - I'm Andre.** I'm passionate about machine learning ü§ñ and better understanding the "black box" that is the neural network. By better understanding implicit biases and the thinking processes üß† of deep learning models, I believe we will better be able to use it in our society.

üìë My resume is [here](https://andre-ye.github.io/scripts/andre-ye-resume.pdf){:target="_blank"}.

üñ®Ô∏è Some of my favorite papers:
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635v1){:target="_blank"}.
- [What‚Äôs Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/pdf/1911.13299.pdf){:target="_blank"}.
- [Scaling description of generalization with number of parameters in deep learning](https://arxiv.org/pdf/1901.01608.pdf){:target="_blank"}.
- [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/pdf/1812.11118.pdf){:target="_blank"}.
- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530){:target="_blank"}.

üìà I write articles about machine learning, data science, and occasionally mathematics [here](https://andre-ye.medium.com/){:target="_blank"}. Some of my favorite articles:
- [The Beauty of Bayesian Optimization, Explained in Simple Terms](https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f){:target="_blank"}
- [Obtaining Top Neural Network Performance Without Any Training](https://medium.com/analytics-vidhya/obtaining-top-neural-network-performance-without-any-training-5af0af464c59){:target="_blank"}
- [If You're Hyped About GPT-3 Writing Code, You Haven‚Äôt Heard of NAS](https://towardsdatascience.com/if-youre-hyped-about-gpt-3-writing-code-you-haven-t-heard-of-nas-19c8c30fcc8a){:target="_blank"}
- [The Clever Trick Behind Google‚Äôs InceptionNet: The 1√ó1 Convolution?](https://towardsdatascience.com/the-clever-trick-behind-googles-inception-the-1-1-convolution-58815b20113){:target="_blank"}
- [Long Short-Term Memory Networks Are Dying: What‚Äôs Replacing It?](https://towardsdatascience.com/long-short-term-memory-networks-are-dying-whats-replacing-it-5ff3a99399fe){:target="_blank"}
- [Finally, an intuitive explanation of why ReLU works](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792){:target="_blank"}
- [The Fascinating No-Gradient Approach to Neural Net Optimization](https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97){:target="_blank"}
I've also guest-written on other platforms, including [KDnuggets](https://www.kdnuggets.com/2021/01/ultimate-scikit-learn-machine-learning-cheatsheet.html){:target="_blank"}, [Netpune.ai](https://neptune.ai/blog/author/andre-ye){:target="_blank"}, and [Experfy](https://www.experfy.com/blog/author/andre-ye/){:target="_blank"}, among other platforms.

Two of my articles on KDnuggets, an organization that awards the "Nobel Prize of Data Science" with 700+ visitors, recieved silver and gold badges for being top-viewed and top-shared on the platform.
<a href="https://www.kdnuggets.com/2020/12/top-stories-2020-nov.html"><img src="https://www.kdnuggets.com/images/tkb-2011-s.png" width=96 alt="Silver Blog" align="right"></a><a href="https://www.kdnuggets.com/2021/02/top-news-week-0125-0131.html"><img src="https://www.kdnuggets.com/images/tkb-2101-g.png" width=96 alt="Gold Blog" align="right"></a>


Fun fact: one of my articles was cited as a reference for a [Wikipedia page](https://en.wikipedia.org/wiki/LightGBM){:target="_blank"} on the LGM gradient boosting framework.

‚å®Ô∏è Check out [Critiq](https://critiq.tech){:target="_blank"}, a site [Carter Chan-Nui](https://www.linkedin.com/in/carterchannui/){:target="_blank"}, [Om Shah](https://www.linkedin.com/in/om-shah-5a0b571ab/){:target="_blank"}, and I coded to reimagine what peer revision for essays can be and do.


[This page is a work in progress.]
