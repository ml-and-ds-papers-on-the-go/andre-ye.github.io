[Home](https://andre-ye.github.io/) > About

## About ğŸ¤™

**Hey ğŸ‘‹ - I'm Andre.** I'm passionate about machine learning ğŸ¤– and better understanding the "black box" that is neural networks. By better understanding implicit biases and the thinking processes ğŸ§  of deep learning models, we will better be able to use it in our society.

ğŸ“‘ My resume is [here](https://andre-ye.github.io/scripts/andre-ye-resume.pdf){:target="_blank"}.

ğŸ–¨ï¸ Some of my favorite papers:
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635v1){:target="_blank"}.
- [Whatâ€™s Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/pdf/1911.13299.pdf){:target="_blank"}.
- [Scaling description of generalization with number of parameters in deep learning](https://arxiv.org/pdf/1901.01608.pdf){:target="_blank"}.
- [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/pdf/1812.11118.pdf){:target="_blank"}.
- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530){:target="_blank"}.

ğŸ“ˆ I write articles about machine learning, data science, and occasionally mathematics [here](https://andre-ye.medium.com/){:target="_blank"}. Some of my favorite articles:
- [The Beauty of Bayesian Optimization, Explained in Simple Terms)[https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f){:target="_blank"}
- [Obtaining Top Neural Network Performance Without Any Training](https://medium.com/analytics-vidhya/obtaining-top-neural-network-performance-without-any-training-5af0af464c59){:target="_blank"}
- [If You're Hyped About GPT-3 Writing Code, You Havenâ€™t Heard of NAS](https://towardsdatascience.com/if-youre-hyped-about-gpt-3-writing-code-you-haven-t-heard-of-nas-19c8c30fcc8a){:target="_blank"}
- [The Clever Trick Behind Googleâ€™s InceptionNet: The 1Ã—1 Convolution?](https://towardsdatascience.com/the-clever-trick-behind-googles-inception-the-1-1-convolution-58815b20113){:target="_blank"}
- [Long Short-Term Memory Networks Are Dying: Whatâ€™s Replacing It?](https://towardsdatascience.com/long-short-term-memory-networks-are-dying-whats-replacing-it-5ff3a99399fe){:target="_blank"}
- [Finally, an intuitive explanation of why ReLU works](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792){:target="_blank"}
- [The Fascinating No-Gradient Approach to Neural Net Optimization](https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97){:target="_blank"}

âŒ¨ï¸ Check out [Critiq](https://critiq.tech){:target="_blank"}, a site [Carter Chan-Nui](https://www.linkedin.com/in/carterchannui/){:target="_blank"}, [Om Shah](https://www.linkedin.com/in/om-shah-5a0b571ab/){:target="_blank"}, and I coded to reimagine what peer revision for essays can be and do.

[This page is a work in progress.]
