[Back](https://andre-ye.github.io/)

<br>

---

<br>

Hey - I'm Andre. I'm passionate about machine learning and better understanding the "black box" that is neural networks. By better understanding implicit biases and the thinking processes of deep learning models, we will better be able to use it in our society.

My resume is [here](https://andre-ye.github.io/scripts/andre-ye-resume.pdf){:target="_blank"}.

Some of my favorite papers:
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635v1){:target="_blank"}.
- [What’s Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/pdf/1911.13299.pdf){:target="_blank"}.
- [Scaling description of generalization with number of parameters in deep learning](https://arxiv.org/pdf/1901.01608.pdf){:target="_blank"}.
- [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/pdf/1812.11118.pdf){:target="_blank"}.
- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530){:target="_blank"}.

I write articles about machine learning, data science, and occasionally mathematics [here](https://andre-ye.medium.com/){:target="_blank"}. Some of my favorite articles:
- [The Beauty of Bayesian Optimization, Explained in Simple Terms: The intuition behind an ingenious algorithm](https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f){:target="_blank"}
- [Obtaining Top Neural Network Performance Without Any Training: We know less about NNs than we thought](https://medium.com/analytics-vidhya/obtaining-top-neural-network-performance-without-any-training-5af0af464c59){:target="_blank"}
- [If You're Hyped About GPT-3 Writing Code, You Haven’t Heard of NAS: Automating AI discovery & research](https://towardsdatascience.com/if-youre-hyped-about-gpt-3-writing-code-you-haven-t-heard-of-nas-19c8c30fcc8a){:target="_blank"}
- [The Clever Trick Behind Google’s InceptionNet: The 1×1 Convolution: What does a 1×1 conv even do?](https://towardsdatascience.com/the-clever-trick-behind-googles-inception-the-1-1-convolution-58815b20113){:target="_blank"}
- [Long Short-Term Memory Networks Are Dying: What’s Replacing It?: The rise and fall of the LSTM](https://towardsdatascience.com/long-short-term-memory-networks-are-dying-whats-replacing-it-5ff3a99399fe){:target="_blank"}
- [Finally, an intuitive explanation of why ReLU works: How a piecewise linear function adds nonlinearity](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792){:target="_blank"}
- [The Fascinating No-Gradient Approach to Neural Net Optimization: Forget Adam, Adagrad, SGD](https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97){:target="_blank"}

[This page is a work in progress.]
